// Module included in the following assemblies:
// * edge_computing/image-based-upgrade/cnf-preparing-for-image-based-upgrade.adoc

:_mod-docs-content-type: PROCEDURE
[id="zp-image-based-upgrade-installing-operators-with-gitops_{context}"]
= Installing {lcao} and the OADP Operator with ZTP GitOps

You need both the {lcao} and the OADP Operator to do an image-based upgrade with ZTP GitOps.

[id="zp-image-based-upgrade-installing-lcao-with-gitops_{context}"]
== Installing the {lcao} with GitOps ZTP

Install the {lcao} with GitOps ZTP to do an image-based upgrade.

.Prerequisites

* Create a directory called `custom-crs` in the `source-crs` directory. The `source-crs` directory must be located in the same location as `kustomization.yaml` file.

.Procedure

. Create the following CRs in the `openshift-lifecycle-agent` namespace and push them to the `source-crs/custom-crs` directory.
+
--
.LcaSubscriptionNS.yaml
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-lifecycle-agent
  annotations:
    workload.openshift.io/allowed: management
    ran.openshift.io/ztp-deploy-wave: "2"
  labels:
    kubernetes.io/metadata.name: openshift-lifecycle-agent
----

.LcaSubscriptionOperGroup.yaml
[source,yaml]
----
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: lifecycle-agent-operatorgroup
  namespace: openshift-lifecycle-agent
  annotations:
    ran.openshift.io/ztp-deploy-wave: "2"
spec:
  targetNamespaces:
    - openshift-lifecycle-agent
----

.LcaSubscription.yaml
[source,yaml]
----
apiVersion: operators.coreos.com/v1
kind: Subscription
metadata:
  name: lifecycle-agent
  namespace: openshift-lifecycle-agent
  annotations:
    ran.openshift.io/ztp-deploy-wave: "2"
spec:
  channel: "stable"
  name: lifecycle-agent
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  installPlanApproval: Manual
----

.Example directory structure
[source,terminal]
----
├── kustomization.yaml
├── sno
│   ├── example-cnf.yaml
│   ├── common-ranGen.yaml
│   ├── group-du-sno-ranGen.yaml
│   ├── group-du-sno-validator-ranGen.yaml
│   └── ns.yaml
├── source-crs
│   ├── custom-crs
│   │   ├── LcaSubscriptionNS.yaml
│   │   ├── LcaSubscriptionOperGroup.yaml
│   │   ├── LcaSubscription.yaml
----
--

. Add the CRs to your common `PolicyGenTemplate`.
+
.Example directory structure
[source,yaml]
----
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "example-common-latest"
  namespace: "ztp-common"
spec:
  bindingRules:
    common: "true"
    du-profile: "latest"
  sourceFiles:
    - fileName: custom-crs/LcaSubscriptionNS.yaml
      policyName: "subscriptions-policy"
    - fileName: custom-crs/LcaSubscriptionOperGroup.yaml
      policyName: "subscriptions-policy"
    - fileName: custom-crs/LcaSubscription.yaml
      policyName: "subscriptions-policy"
[...]
----

[id="zp-image-based-upgrade-installing-oadp-with-gitops_{context}"]
== Installing and configuring the OADP Operator with GitOps ZTP

You can install and configure the OADP Operator with GitOps ZTP well before you want to start the upgrade.

.Prerequisites

* Create a directory called `custom-crs` in the `source-crs` directory. The `source-crs` directory must be located in the same location as `kustomization.yaml` file.

.Procedure

. Create the following CRs in the `openshift-adp` namespace and push them to the `source-crs/custom-crs` directory.
+
--
.OadpSubscriptionNS.yaml
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-adp
  annotations:
    workload.openshift.io/allowed: management
    ran.openshift.io/ztp-deploy-wave: "2"
  labels:
    kubernetes.io/metadata.name: openshift-adp
----

.OadpSubscriptionOperGroup.yaml
[source,yaml]
----
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: redhat-oadp-operator
  namespace: openshift-adp
  annotations:
    ran.openshift.io/ztp-deploy-wave: "2"
spec:
  targetNamespaces:
  - openshift-adp
----

.OadpSubscription.yaml
[source,yaml]
----
apiVersion: operators.coreos.com/v1
kind: Subscription
metadata:
  name: redhat-oadp-operator
  namespace: openshift-adp
  annotations:
    ran.openshift.io/ztp-deploy-wave: "2"
spec:
  channel: stable-1.3
  name: redhat-oadp-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  installPlanApproval: Manual
status:
  state: AtLatestKnown
----

.Example directory structure
[source,terminal]
----
├── kustomization.yaml
├── sno
│   ├── example-cnf.yaml
│   ├── common-ranGen.yaml
│   ├── group-du-sno-ranGen.yaml
│   ├── group-du-sno-validator-ranGen.yaml
│   └── ns.yaml
├── source-crs
│   ├── custom-crs
│   │   ├── OadpSubscriptionNS.yaml
│   │   ├── OadpSubscriptionOperGroup.yaml
│   │   ├── OadpSubscription.yaml
----
--

. Add the CRs to your common `PolicyGenTemplate`.
+
.Example directory structure
+
[source,yaml]
----
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "example-common-latest"
  namespace: "ztp-common"
spec:
  bindingRules:
    common: "true"
    du-profile: "latest"
  sourceFiles:
    - fileName: custom-crs/OadpSubscriptionNS.yaml
      policyName: "subscriptions-policy"
    - fileName: custom-crs/OadpSubscriptionOperGroup.yaml
      policyName: "subscriptions-policy"
    - fileName: custom-crs/OadpSubscription.yaml
      policyName: "subscriptions-policy"
[...]
----

. Create the `DataProtectionApplication` CR and the S3 secret.

.. Create the following CRs in your `source-crs/custom-crs` directory:
+
--
.DataProtectionApplication.yaml
[source,yaml]
----
apiVersion: oadp.openshift.io/v1
kind: DataProtectionApplication
metadata:
  name: dataprotectionapplication
  namespace: openshift-adp
  annotations:
    ran.openshift.io/ztp-deploy-wave: "100"
spec:
  configuration:
    restic:
      enable: false <1>
    velero:
      defaultPlugins:
        - aws
        - openshift
      resourceTimeout: 10m
  backupLocations:
    - velero:
        config:
          profile: "default"
          region: minio
          s3Url: $url
          insecureSkipTLSVerify: "true"
          s3ForcePathStyle: "true"
        provider: aws
        default: true
        credential:
          key: cloud
          name: cloud-credentials
        objectStorage:
          bucket: $bucketName
          prefix: $prefixName
status:
  conditions:
  - reason: Complete
    status: "True"
    type: Reconciled
----
<1> The `spec.configuration.restic.enable` field must be set to `false` for an image-based upgrade because persistent volume contents are retained and reused after the upgrade.

.OadpSecret.yaml
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: cloud-credentials
  namespace: openshift-adp
  annotations:
    ran.openshift.io/ztp-deploy-wave: "100"
type: Opaque
----

.OadpBackupStorageLocationStatus.yaml
[source,yaml]
----
apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
  namespace: openshift-adp
  annotations:
    ran.openshift.io/ztp-deploy-wave: "100"
status:
  phase: Available
----

The `OadpBackupStorageLocationStatus.yaml` CR verifies the availability of backup storage locations created by OADP.
--

.. Add the CRs to your site `PolicyGenTemplate` with overrides.
+
[source,yaml]
----
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "example-cnf"
  namespace: "ztp-site"
spec:
  bindingRules:
    sites: "example-cnf"
    du-profile: "latest"
  mcp: "master"
  sourceFiles:
    ...
    - fileName: OadpSecret.yaml
      policyName: "config-policy"
      data:
        cloud: <your_credentials> <1>
    - fileName: DataProtectionApplication.yaml
      policyName: "config-policy"
      spec:
        backupLocations:
          - velero:
              config:
                region: minio
                s3Url: <your_S3_URL> <2>
              objectStorage:
                bucket: <your_bucket_name> <3>
                prefix: <cluster-name> <3>
    - fileName: OadpBackupStorageLocationStatus.yaml
      policyName: "config-policy"
----
<1> Specify your credentials for your S3 storage backend.
<2> Specify the URL for your S3-compatible bucket.
<3> The `bucket` defines the bucket name that is created in S3 backend. The `prefix` defines the name of the subdirectory that will be automatically created in the `bucket`. The combination of `bucket` and `prefix` must be unique for each target cluster to avoid interference between them. To ensure a unique storage directory for each target cluster, you can use the {rh-rhacm} hub template function, for example, `prefix: {{hub .ManagedClusterName hub}}`.